# Application de chat avec un LLM pour l'école des Ponts

## Prérequis :

- Installation d'**Ollama** avec le modèle **mistral-small:latest**
- Installation de Node.js

## Pour tester en local :

Installer les dépendances :
### `npm install`

Lancer le service ollama avec le bon modèle de langage :
### `ollama serve`

Finalement pour démarrer l'application : 
### `npm start`